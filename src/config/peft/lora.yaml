name: "lora"
ce_loss_weight: 1
align_loss_weight: 0
lr: 5e-4
r: 16
lora_dropout: 0.05

idefics-9b:
  task_type: CAUSAL_LM
  r: ${..r}
  target_modules: ["q_proj", "k_proj", "o_proj", "v_proj"]
  lora_alpha: ${eval:'${..r} * 2'}
  lora_dropout: ${..lora_dropout}

idefics2-8b-base:
  task_type: CAUSAL_LM
  r: ${..r}
  target_modules: ["q_proj", "k_proj", "o_proj", "v_proj"] 
  lora_alpha: ${eval:'${..r} * 2'}
  lora_dropout: ${..lora_dropout}

llava-interleave-7b:
  task_type: CAUSAL_LM
  r: ${..r}
  target_modules: ["q_proj", "k_proj", "o_proj", "v_proj"] 
  lora_alpha: ${eval:'${..r} * 2'}
  lora_dropout: ${..lora_dropout}

llama-7b:
  task_type: CAUSAL_LM
  r: ${..r}
  target_modules: ["q_proj", "k_proj", "o_proj", "v_proj"]
  lora_alpha: ${eval:'${..r} * 2'}
  lora_dropout: ${..lora_dropout}